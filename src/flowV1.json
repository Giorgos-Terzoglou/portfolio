{
  "meta": {
    "version": "v1",
    "botName": "Terzo",
    "modeDefault": "recruiter",
    "modes": [
      "recruiter",
      "technical"
    ],
    "primaryRule": "User is interviewer. Terzo answers. Navigation is buttons + topic chips. Typed input is optional but routes only via explicit intents (no embeddings).",
    "evidenceRule": "Never show more than 6 visuals at once. Videos only in deep-dive nodes."
  },
  "assets": {
    "basePath": "",
    "projects": {
      "autonomous_driving": {
        "md": "autonomous_driving/autonomous_driving_story.md",
        "images": {
          "lane": "autonomous_driving/lane_detection&steering_advice.png",
          "signs_cls": "autonomous_driving/signs_classification.png",
          "signs_det": "autonomous_driving/signs_detection.png",
          "tl_cls": "autonomous_driving/traffic_light_classification.png",
          "dist": "autonomous_driving/detection&distance.png",
          "drive_seg": "autonomous_driving/drivable_area_segmentation.png",
          "night_dificult": "autonomous_driving/night_dificult.png"
        },
        "videos": {
          "trafficSigns": "autonomous_driving/trafficSigns.mp4",
          "trafficSignsAndDistances": "autonomous_driving/trafficSigns_and_distances.mp4"
        },
        "links": {
          "repo": "https://github.com/Giorgos-Terzoglou/Convolutional-neural-network-for-autonomous-driving-in-Python"
        }
      },
      "solar_panel_defects": {
        "md": "solar_panel_defects/solar_panel_defect_detection_story.md",
        "images": {
          "workflow": "solar_panel_defects/workflow_pipeline.png",
          "panel_det": "solar_panel_defects/panel_detection.png",
          "panel_seg": "solar_panel_defects/panel_segmentation.png",
          "panel_det_full": "solar_panel_defects/panel_detection_full_pipeline.png",
          "panel_cls": "solar_panel_defects/panel_classification.png"
        },
        "links": {
          "paper": "https://ieeexplore.ieee.org/document/10167960"
        }
      },
      "traffic_technique": {
          "md": "traffic_technique/traffic_technique_story.md",
          "links": {
            "company": "https://www.traffictech.gr"
          }
        },
        "interpreter": {
          "links": {
            "website": "https://www.interpreter-h2020.eu/"
          }
        },
        "mementoes": {
          "links": {
            "website": "https://mementoes.eu/#"
          }
        }
      },
    "globalLinks": {
      "cv_pdf": "CV_Georgios_Terzoglou.pdf"
    }
  },
  "ui": {
    "topicChips": [
      {
        "id": "chip_experience",
        "label": "Experience",
        "goto": "n_exp_01",
        "utterance": "Can you walk me through your experience and what you actually owned end-to-end?"
      },
      {
        "id": "chip_projects",
        "label": "Projects",
        "goto": "n_proj_01",
        "utterance": "Show me your flagship projects with proof."
      },
      {
        "id": "chip_publications",
        "label": "Publications",
        "goto": "n_pubs_01",
        "utterance": "Which publications validate your systems, and what do they contribute?"
      },
      {
        "id": "chip_education",
        "label": "Education",
        "goto": "n_edu_01",
        "utterance": "What’s your academic background and thesis focus?"
      },
      {
        "id": "chip_contact",
        "label": "Contact",
        "goto": "n_contact_01",
        "utterance": "How can I contact you, and where can I see your work?"
      }
    ],
    "alwaysVisibleControls": [
      "repeat_question",
      "change_answer",
      "timeline",
      "switch_mode",
      "switch_project",
      "contact"
    ]
  },
  "intents": {
    "routingRules": [
      {
        "intent": "projects",
        "keywords": [
          "project",
          "projects",
          "work",
          "portfolio"
        ],
        "goto": "n_proj_01"
      },
      {
        "intent": "skills",
        "keywords": [
          "skills",
          "stack",
          "tools",
          "tech"
        ],
        "goto": "n_skills_01"
      },
      {
        "intent": "experience",
        "keywords": [
          "experience",
          "roles",
          "jobs",
          "certh",
          "quento",
          "traffic"
        ],
        "goto": "n_exp_01"
      },
      {
        "intent": "publications",
        "keywords": [
          "paper",
          "publication",
          "ieee",
          "research"
        ],
        "goto": "n_pubs_01"
      },
      {
        "intent": "education",
        "keywords": [
          "education",
          "degree",
          "msc",
          "thesis"
        ],
        "goto": "n_edu_01"
      },
      {
        "intent": "contact",
        "keywords": [
          "contact",
          "email",
          "linkedin",
          "github"
        ],
        "goto": "n_contact_01"
      },
      {
        "intent": "summary",
        "keywords": [
          "summary",
          "overview",
          "about",
          "who are you"
        ],
        "goto": "n_summary_01"
      }
    ],
    "fallback": {
      "reply": "Pick a topic and I’ll answer like you’re interviewing me.",
      "showChips": true
    }
  },
  "nodes": [
    {
      "id": "n_summary_01",
      "type": "answer",
      "section": "summary",
      "bot": "I’m a Machine Learning Engineer focused on building ML systems that run in production — especially computer vision, NLP/RAG, time-series forecasting, and MLOps. I’ve worked across research (CERTH, peer-reviewed IEEE work) and industry (production pipelines, deployed systems).",
      "options": [
        {
          "id": "o_sum_projects",
          "label": "Flagship projects",
          "utterance": "Show me your flagship projects.",
          "goto": "n_proj_01"
        },
        {
          "id": "o_sum_exp",
          "label": "Walk through experience",
          "goto": "n_exp_01",
          "utterance": "Can you tell me about your previous experience."
        },
        {
          "id": "show_publications",
          "label": "Show publications",
          "goto": "n_pubs_01",
          "utterance": "Show me some of your publications."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "CV (PDF)",
          "src": "CV_Georgios_Terzoglou.pdf"
        },
        {
          "type": "link",
          "label": "GitHub",
          "src": "https://github.com/Giorgos-Terzoglou"
        },
        {
          "type": "link",
          "label": "LinkedIn",
          "src": "https://www.linkedin.com/in/georgios-terzoglou-792722348/"
        }
      ]
    },
    {
      "id": "n_exp_01",
      "type": "question",
      "section": "experience",
      "bot": "Which part of my experience do you want to dig into?",
      "options": [
        {
          "id": "o_exp_quento",
          "label": "Quento (RAG + production pipelines)",
          "goto": "n_exp_quento_01",
          "utterance": "Can you tell more about your experience in Quento?"
        },
        {
          "id": "o_exp_certh",
          "label": "CERTH (EU R&D → real delivery)",
          "goto": "n_exp_certh_01",
          "utterance": "Can you tell more about your experience in CERTH?"
        },
        {
          "id": "o_exp_tt",
          "label": "Traffic Technique (time-series + deployment)",
          "goto": "n_exp_tt_01",
          "utterance": "Can you tell more about your experience in Traffic Technique?"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_exp_quento_01",
      "type": "answer",
      "section": "experience",
      "bot": "At Quento, I work on production ML systems where the hard part isn’t the model — it’s everything around it.\n\nWhat I do\n• Build and maintain end-to-end pipelines for client-facing systems, mostly RAG chatbots.\n• Treat RAG as an engineering problem: retrieval quality, guardrails, latency, and failure modes.\n\nProduction realities\n• Translate ambiguous client requests into concrete system requirements.\n• Make cost / latency / quality trade-offs and push back on unrealistic expectations.\n• Ensure predictable behavior under messy data, tight latency, and reliability constraints.",
      "options": [
        {
          "id": "o_quento_back_exp",
          "label": "Back to experience",
          "goto": "n_exp_01",
          "utterance": "Back to experience."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "CV (PDF)",
          "src": "CV_Georgios_Terzoglou.pdf"
        },
        {
          "type": "link",
          "label": "GitHub profile",
          "src": "https://github.com/Giorgos-Terzoglou"
        }
      ]
    },
    {
      "id": "n_exp_certh_01",
      "type": "answer",
      "section": "experience",
      "bot": "CERTH — I worked across EU-funded R&D where the job wasn’t “train a model”, it was to deliver usable systems.\n\nScope\n• Computer Vision: drone-based industrial inspection (RGB + thermal), detection/segmentation/classification, and reliability under messy real capture.\n• NLP/RAG: retrieval pipelines and chatbot-style systems with correctness/guardrails.\n• Tabular ML + time series: predictive modeling where non-stationarity and drift are the real problems.\n\nResearch + delivery\n• I produced peer-reviewed outputs, but I wasn’t stuck in paper-land.\n• I collaborated with industry partners (Samsung, TITAN) to translate prototypes into deployable product-grade components: constraints, latency, robustness, integration, and failure handling.\n\nIf you want proof: use Projects/Publications — they connect to concrete artifacts.",
      "options": [
        {
          "id": "o_certh_back_exp",
          "label": "Back to experience",
          "goto": "n_exp_01",
          "utterance": "Back to experience."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "PVGnosis project site (EU project)",
          "src": "https://pvgnosis.eu/"
        },
        {
          "type": "link",
          "label": "MEMENTOES project site (EU project)",
          "src": "https://mementoes.eu/#"
        },
        {
          "type": "link",
          "label": "INTERPRETER project site (H2020)",
          "src": "https://www.interpreter-h2020.eu/"
        },
        {
          "type": "link",
          "label": "IEEE publication (Solar Panel Defect Detection)",
          "src": "https://ieeexplore.ieee.org/document/10167960"
        },
        {
          "type": "link",
          "label": "IEEE publication (Game Assets services)",
          "src": "https://ieeexplore.ieee.org/abstract/document/10976973"
        },
        {
          "type": "link",
          "label": "Elsevier publication (Digital heritage digital twins)",
          "src": "https://www.sciencedirect.com/science/article/pii/S2212054825000530"
        }
      ]
    },
    {
      "id": "n_exp_tt_01",
      "type": "answer",
      "section": "experience",
      "bot": "At Traffic Technique, the company’s goal is to adapt traffic light behavior based on real traffic flows instead of static timing plans.\n\nI built and owned an end-to-end adaptive traffic prediction system that moved from experimentation into core company infrastructure.\n\nWhat I did\n• Designed the full pipeline: data ingestion → preprocessing → forecasting → evaluation → deployment.\n• Implemented time-series models to predict how many vehicles will pass through each road segment.\n• Improved prediction accuracy compared to static and heuristic approaches, enabling better signal timing decisions.\n\nProduction realities\n• Set up monitoring to detect performance decay and data drift.\n• Implemented automated retraining and CI/CD-style pipelines to keep models reliable over time.\n• Supported scaling from a small local deployment to hundreds of nodes across multiple cities, contributing to reduced congestion.",
      "options": [
        {
          "id": "o_tt_back_exp",
          "label": "Back to experience",
          "goto": "n_exp_01",
          "utterance": "Back to experience."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Traffic Technique (company)",
          "src": "https://www.traffictech.gr"
        }
      ]
    },
    {
      "id": "n_edu_01",
      "type": "answer",
      "section": "education",
      "bot": "I hold an MSc in Electrical & Computer Engineering.\n\nThesis: CNN-based autonomous driving — from perception (lanes/signs/lights) to actionable driving decisions, packaged as a working end-to-end prototype.",
      "options": [
        {
          "id": "o_edu_thesis",
          "label": "Show the Autonomous Driving project",
          "goto": "n_proj_auto_overview_01",
          "utterance": "Can you tell me more about your autonomous driving thesis project?"
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Autonomous Driving Repo",
          "src": "https://github.com/Giorgos-Terzoglou/Convolutional-neural-network-for-autonomous-driving-in-Python"
        }
      ]
    },
    {
      "id": "n_pubs_01",
      "type": "question",
      "section": "publications",
      "bot": "These are applied publications tied to real systems. Pick one and I’ll explain what problem it solves and why it matters.",
      "options": [
        {
          "id": "o_pubs_game_assets",
          "label": "AI Services for Customizable Game Assets (IEEE)",
          "goto": "n_pub_game_assets_01",
          "utterance": "Tell me about the AI services for customizable game assets paper."
        },
        {
          "id": "o_pubs_digital_twins",
          "label": "Digital Heritage & Digital Twins (Elsevier)",
          "goto": "n_pub_mementoes_digital_twins_01",
          "utterance": "Tell me about the digital heritage digital twins paper."
        },
        {
          "id": "o_pubs_solar_dsp_ieee",
          "label": "Solar Panel Defect Detection (DSP 2023 / IEEE)",
          "goto": "n_pub_solar_dsp_01",
          "utterance": "Tell me about your DSP 2023 / IEEE solar panel defect detection paper."
        }
      ],
      "evidence": []
    },
    {
      "id": "n_mlop_deploy_01",
      "type": "answer",
      "section": "mlops",
      "bot": "I deploy models as services (typically FastAPI) inside Docker, with clean versioning and reproducible environments. The goal is stable inference, predictable performance, and easy rollback — not notebooks in production.",
      "options": [
        {
          "id": "o_mlop_monitor2",
          "label": "How do you monitor drift/perf?",
          "goto": "n_mlop_monitor_01"
        },
        {
          "id": "o_mlop_projects",
          "label": "Show deployed-style projects",
          "goto": "n_proj_tt_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_mlop_stack_01",
      "type": "answer",
      "section": "mlops",
      "bot": "Core tools: Python, PyTorch, FastAPI, Docker, MLflow, Git/GitHub Actions, and cloud tooling (e.g., Azure/Databricks depending on project constraints). I choose tools that maximize reproducibility and operational stability.",
      "options": [
        {
          "id": "o_stack_projects",
          "label": "Show projects using this stack",
          "goto": "n_proj_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_proj_01",
      "type": "question",
      "section": "projects",
      "bot": "Pick a project. I’ll keep it tight and show proof.",
      "options": [
        {
          "id": "o_proj_auto",
          "label": "Autonomous Driving (camera-only)",
          "goto": "n_proj_auto_overview_01",
          "utterance": "Can you please tell me more about your autonomous driving project?"
        },
        {
          "id": "o_proj_solar",
          "label": "PVGnosis (solar panel defect detection)",
          "goto": "n_proj_solar_overview_01",
          "utterance": "Can you please tell me more about your solar panel defect detection project?"
        },
        {
          "id": "o_proj_tt",
          "label": "Adaptive Traffic Control (production)",
          "goto": "n_proj_tt_overview_01",
          "utterance": "Can you please tell me more about your adaptive traffic control project?"
        },
        {
          "id": "o_proj_interpreter",
          "label": "INTERPRETER (H2020 — smart grid tools)",
          "goto": "n_proj_interpreter_overview_01",
          "utterance": "Can you tell me what you did in INTERPRETER?"
        },
        {
          "id": "o_proj_mementoes",
          "label": "MEMENTOES (digital twins + restoration AI)",
          "goto": "n_proj_mementoes_overview_01",
          "utterance": "Can you tell me what you built in MEMENTOES?"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_proj_auto_01",
      "type": "question",
      "section": "projects.autonomous_driving",
      "bot": "Autonomous driving project — what do you want first?",
      "options": [
        {
          "id": "o_auto_overview_menu",
          "label": "Overview",
          "goto": "n_proj_auto_overview_01",
          "utterance": "Give me the overview."
        },
        {
          "id": "o_auto_modules_menu",
          "label": "Modules",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Walk me through the modules."
        },
        {
          "id": "o_auto_limits_menu",
          "label": "Limitations",
          "goto": "n_proj_auto_limits_01",
          "utterance": "What are the limitations?"
        },
        {
          "id": "o_auto_demo_menu",
          "label": "Show demo videos",
          "goto": "n_proj_auto_demo_01",
          "utterance": "Show me the demo videos."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Repo",
          "src": "https://github.com/Giorgos-Terzoglou/Convolutional-neural-network-for-autonomous-driving-in-Python"
        }
      ]
    },
    {
      "id": "n_proj_auto_overview_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Autonomous Driving (camera-only) — a real-time-ish perception pipeline that turns a single front-camera stream into driving cues.\n\nI built an end-to-end prototype in Python/OpenCV that mixes classical CV geometry with CNN-based classification/detection/segmentation and lightweight rule-based logic.\n\nWhat the system produces:\n• lane position + steering suggestion from monocular lane geometry\n• traffic sign detection + CNN sign classification for downstream rules\n• traffic-light state classification (red/yellow/green)\n• object detection plus rough distance estimation from bbox scale (monocular heuristic)\n• a drivable-area mask (road vs non-road) to support “stay on road” decisions\n\nIt’s designed to fail loudly: confidence thresholds, sanity checks, and outputs that degrade gracefully instead of pretending certainty.",
      "options": [
        {
          "id": "o_auto_modules",
          "label": "Modules",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Can you walk me through the modules in this project?"
        },
        {
          "id": "o_auto_limits_new",
          "label": "Limitations",
          "goto": "n_proj_auto_limits_01",
          "utterance": "What were the limitations of this system?"
        },
        {
          "id": "o_auto_demo_new",
          "label": "Show demo videos",
          "goto": "n_proj_auto_demo_01",
          "utterance": "Can you show me demo videos?"
        },
        {
          "id": "o_interpreter_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Repo",
          "src": "https://github.com/Giorgos-Terzoglou/Convolutional-neural-network-for-autonomous-driving-in-Python"
        },
        {
          "type": "video",
          "label": "Signs + distance estimation (demo)",
          "src": "/autonomous_driving/trafficSigns_and_distances.mp4"
        }
      ]
    },
    {
      "id": "n_proj_auto_perception_01",
      "type": "question",
      "section": "projects.autonomous_driving",
      "bot": "Which module do you want me to break down?",
      "options": [
        {
          "id": "o_auto_lane",
          "label": "Lane detection & steering",
          "goto": "n_proj_auto_lane_01",
          "utterance": "Tell me about the lane detection + steering module."
        },
        {
          "id": "o_auto_signs",
          "label": "Traffic signs & lights",
          "goto": "n_proj_auto_signs_01",
          "utterance": "Tell me about traffic signs and traffic lights."
        },
        {
          "id": "o_auto_dist",
          "label": "Object detection + distance",
          "goto": "n_proj_auto_dist_01",
          "utterance": "Explain object detection and distance estimation."
        },
        {
          "id": "o_auto_seg",
          "label": "Drivable area segmentation",
          "goto": "n_proj_auto_seg_01",
          "utterance": "Explain drivable-area segmentation."
        }
      ],
      "evidence": []
    },
    {
      "id": "n_proj_auto_lane_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Lane detection + steering advice\n\nThis module estimates where the car sits inside the lane and outputs a simple steering cue (keep center / drift left / drift right) using a monocular, camera-only pipeline.\n\nUnder the hood it’s classical CV with geometry:\n• color + gradient thresholding, edge extraction, and ROI masking\n• perspective assumptions to stabilize the lane region\n• line fitting (Hough / polynomial fit) to estimate lane boundaries\n• lane-center offset and curvature turned into a rule-based steering suggestion\n\nIt’s fast, explainable, and easy to debug — but it’s also brittle because it depends on visible lane paint and stable lighting.",
      "options": [
        {
          "id": "o_lane_another",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_lane_limits",
          "label": "Hard parts & limitations",
          "goto": "n_proj_auto_lane_limits_01",
          "utterance": "What were the hard parts and limitations for lane detection?"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Lane → steering suggestion",
          "src": "autonomous_driving/lane_detection&steering_advice.png"
        }
      ]
    },
    {
      "id": "n_proj_auto_signs_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Traffic signs + traffic lights\n\nTraffic signs (detect + classify)\nI detect sign candidates, crop ROIs, and run CNN classification to output sign label + confidence. This is tuned for small objects, so it relies on clean crops, confidence thresholds, and basic smoothing to avoid flicker.\n\nTraffic lights (state classification)\nFrom cropped regions, explicit color-based classification used for the light state (red / yellow / green). Simple temporal smoothing helps stabilize predictions frame-to-frame.\n\nThis pair is the “semantic layer” of the prototype: it converts pixels into discrete decisions that downstream rules can actually use.",
      "options": [
        {
          "id": "o_signs_another",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_signs_limits",
          "label": "Hard parts & limitations",
          "goto": "n_proj_auto_signs_limits_01",
          "utterance": "What were the hard parts and limitations for signs/lights?"
        }
      ],
      "evidence": [
        {
          "type": "video",
          "label": "Traffic sign detection - classification",
          "src": "autonomous_driving/trafficSigns.mp4"
        },
        {
          "type": "image",
          "label": "Traffic sign classification",
          "src": "autonomous_driving/signs_classification.png"
        },
        {
          "type": "image",
          "label": "Traffic light classification",
          "src": "autonomous_driving/traffic_light_classification.png"
        }
      ]
    },
    {
      "id": "n_proj_auto_dist_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Object detection + monocular distance estimation\n\nWhat it does\nDetect objects and estimate distance using a camera-only heuristic.\n\nHow it works\n• Detector produces bounding boxes per object.\n• Distance is approximated from bbox size (e.g., width/height) assuming a fixed camera model and nominal object size.\n\nWhy it’s useful\nGood enough for a demo / sanity checks, but not reliable for real autonomy.\n\nKeywords\nmonocular distance heuristic, bbox geometry, camera FOV sensitivity, calibration assumptions.",
      "options": [
        {
          "id": "o_dist_another",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_dist_limits",
          "label": "Hard parts & limitations",
          "goto": "n_proj_auto_dist_limits_01",
          "utterance": "What were the hard parts and limitations for distance estimation?"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Detection + distance estimation",
          "src": "autonomous_driving/detection&distance.png"
        },
        {
          "type": "video",
          "label": "Signs + distance estimation (demo)",
          "src": "autonomous_driving/trafficSigns_and_distances.mp4"
        }
      ]
    },
    {
      "id": "n_proj_auto_seg_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Drivable-area segmentation\n\nThis module produces a pixel-level road mask (drivable vs non-drivable) so the system can reason about “stay on road” even when lane lines are missing.\n\nPipeline:\n• run a semantic segmentation model (road vs background)\n• post-process the mask with morphology / smoothing to reduce noise\n• output a clean drivable-area mask for downstream safety logic\n\nIt’s the part that tries to survive messy roads — but camera-only segmentation still struggles with harsh lighting, shadows, worn asphalt, and the kind of inconsistent road markings you see a lot in Greece.",
      "options": [
        {
          "id": "o_seg_another",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_seg_limits",
          "label": "Hard parts & limitations",
          "goto": "n_proj_auto_seg_limits_01",
          "utterance": "What were the hard parts and limitations for drivable-area segmentation?"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Drivable area segmentation",
          "src": "autonomous_driving/drivable_area_segmentation.png"
        }
      ]
    },
    {
      "id": "n_proj_auto_limits_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "System-level limitations (camera-only prototype)\n\n• No sensor fusion (no LiDAR/radar/IMU/GPS) → weak depth and motion understanding.\n• Heuristic distance estimation → unstable under viewpoint/FOV changes.\n• Lighting and weather sensitivity (night, glare, rain).\n• Not a safety-certified stack: demo-grade perception and simple decision rules.",
      "options": [
        {
          "id": "o_limits_modules",
          "label": "Modules",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Can you walk me through the modules?"
        },
        {
          "id": "o_limits_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        }
      ],
      "evidence": []
    },
    {
      "id": "n_proj_auto_demo_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Pick a demo. (Videos only show up here to keep the evidence panel clean.)",
      "options": [
        {
          "id": "o_demo_back_overview",
          "label": "Back to overview",
          "goto": "n_proj_auto_overview_01",
          "utterance": "Back to the overview."
        }
      ],
      "evidence": [
          { "type": "video", "label": "Traffic signs demo", "src": "autonomous_driving/trafficSigns.mp4" },
          { "type": "video", "label": "Signs + distances demo", "src": "/autonomous_driving/trafficSigns_and_distances.mp4" }
      ]
    },
    {
    "id": "n_proj_solar_overview_01",
    "type": "answer",
    "section": "projects.solar_panel_defects",
    "bot": "PVGnosis (solar panel defect detection) — a UAV RGB+thermal inspection pipeline that turns raw flight imagery into panel-level defect decisions.\n\nIt’s intentionally multi-stage: detection → interior isolation → fault classification → sanity checks via thermal statistics. The point isn’t a pretty benchmark — it’s reliability under real flight conditions (perspective distortion, thermal noise, motion blur, alignment errors).\n\nWhat the system produces:\n• per-panel localization (where each panel is)\n• a clean panel-interior mask\n• fault label + confidence\n• explainability heatmaps + thermal consistency checks to avoid “confident nonsense”\n\nIt’s built to fail conservatively: if a stage is uncertain, later checks and fallbacks prevent the pipeline from pretending certainty.",
    "options": [
      {
        "id": "o_solar_pipeline2",
        "label": "Pipeline stages",
        "utterance": "Walk me through the pipeline stages.",
        "goto": "n_proj_solar_pipeline_01"
      },
      {
        "id": "o_solar_limits2",
        "label": "Limitations",
        "utterance": "What are the limitations of this system?",
        "goto": "n_proj_solar_limits_01"
      },
      {
        "id": "o_interpreter_back_projects",
        "label": "Back to projects",
        "goto": "n_proj_01",
        "utterance": "Back to projects."
      }
    ],
    "evidence": [
      {
        "type": "link",
        "label": "Project site",
        "src": "https://pvgnosis.eu/"
      },
      {
        "type": "link",
        "label": "IEEE publication (Solar Panel Defect Detection)",
        "src": "https://ieeexplore.ieee.org/document/10167960"
      },
      {
        "type": "image",
        "label": "Workflow",
        "src": "solar_panel_defects/workflow_pipeline.png"
      }
    ]
    },
    {
      "id": "n_proj_solar_pipeline_01",
      "type": "question",
      "section": "projects.solar_panel_defects",
      "bot": "Pick a stage. I’ll explain what it does, why it exists, and where it breaks.",
      "options": [
        {
          "id": "o_solar_det",
          "label": "Panel detection (YOLOv5)",
          "utterance": "Tell me about the panel detection stage.",
          "goto": "n_proj_solar_det_01"
        },
        {
          "id": "o_solar_seg",
          "label": "Interior isolation (UNet + CV fallback)",
          "utterance": "Explain the segmentation and the CV fallback.",
          "goto": "n_proj_solar_seg_01"
        },
        {
          "id": "o_solar_cls",
          "label": "Fault classification (EfficientNet + heatmaps)",
          "utterance": "Explain the fault classification stage and explainability.",
          "goto": "n_proj_solar_cls_01"
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Project site",
          "src": "https://pvgnosis.eu/"
        },
        {
          "type": "link",
          "label": "IEEE publication (Solar Panel Defect Detection)",
          "src": "https://ieeexplore.ieee.org/document/10167960"
        },
        {
          "type": "image",
          "label": "Workflow",
          "src": "solar_panel_defects/workflow_pipeline.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_det_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "This stage defines what the system considers a valid asset.\n\nEach detected box becomes a single photovoltaic panel that the rest of the pipeline reasons about — geometry, thermal behavior, and fault likelihood all happen at this level.\n\nBecause everything downstream depends on it, detection is tuned to be tolerant rather than brittle: it prioritizes coverage and spatial consistency so later stages can validate or reject results instead of silently failing.",
      "options": [
        {
          "id": "o_solar_backstage",
          "label": "Back to stages",
          "utterance": "Let's discuss another stage.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_det_limits",
          "label": "Hard parts & limitations",
          "utterance": "What were the hard parts and limitations for detection?",
          "goto": "n_proj_solar_det_limits_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Panel detection",
          "src": "solar_panel_defects/panel_detection.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_seg_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "This step extracts the true panel interior so downstream models operate only on trustworthy pixels.\n\nTo achieve this, I built a custom computer-vision pipeline to extract clean panel interiors and generate reliable training masks. That dataset was then used to train a UNet segmentation model, which handles the majority of cases accurately.\n\nWhen the learned model becomes unreliable — due to perspective distortion, glare, or incomplete edges — the system falls back to a geometry-driven CV pipeline (edges, line detection, contour completion, perspective warping). This combination significantly increases robustness and prevents segmentation failures from propagating downstream.",
      "options": [
        {
          "id": "o_solar_backstage2",
          "label": "Back to stages",
          "utterance": "Let's discuss another stage.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_robust3",
          "label": "Robustness",
          "utterance": "Why did you go with this approach?",
          "goto": "n_proj_solar_segm_robust_01"
        },
        {
          "id": "o_solar_seg_limits",
          "label": "Hard parts & limitations",
          "utterance": "What were the hard parts and limitations for segmentation?",
          "goto": "n_proj_solar_seg_limits_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Segmentation pipeline",
          "src": "solar_panel_defects/panel_segmentation.png"
        },
        {
          "type": "image",
          "label": "Full segmentation fallback view",
          "src": "solar_panel_defects/panel_detection_full_pipeline.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_cls_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "Each isolated panel is classified into fault or no-fault and, when anomalous, into specific defect types (cell, multi-cell, diode, multi-diode).\n\nThe classifier is based on EfficientNet and trained on a custom synthetic dataset, created to bridge the gap between clean lab data and noisy real UAV thermal imagery.\n\nBeyond the label, the system produces activation heatmaps that highlight the thermal regions driving the decision. This makes predictions inspectable and helps verify that the model focuses on physically meaningful defect patterns rather than background noise.",
      "options": [
        {
          "id": "o_solar_backstage3",
          "label": "Back to stages",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_cls_limits",
          "label": "Hard parts & limitations",
          "utterance": "What were the hard parts and limitations for classification?",
          "goto": "n_proj_solar_cls_limits_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Thermal → heatmap",
          "src": "solar_panel_defects/panel_classification.png"
        }
      ]
    },    
    {
      "id": "n_proj_tt_overview_01",
      "type": "answer",
      "section": "projects.traffic_technique",
      "bot": "This system supports adaptive traffic signal control using real-time data and learned forecasts.\n\nI built and productionized traffic-flow forecasting models (LSTM and Transformer-based) that predict short-term demand per intersection, reducing MAE by ~12% compared to earlier baselines.\n\nBeyond modeling, I implemented automated retraining and monitoring workflows so performance doesn’t decay over time. The result was a system that scaled from pilot deployments to 45+ intersections, supporting city-wide rollout rather than isolated demos.",
      "options": [
        {
          "id": "o_projects_back3",
          "label": "Back to all projects",
          "goto": "n_proj_01"
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Company site",
          "src": "https://www.traffictech.gr"
        }
      ]
    },
    {
      "id": "n_rag_01",
      "type": "question",
      "section": "rag",
      "bot": "Which RAG part do you want to focus on?",
      "options": [
        {
          "id": "o_rag_retrieval",
          "label": "Retrieval strategy",
          "goto": "n_rag_retrieval_01"
        },
        {
          "id": "o_rag_guardrails",
          "label": "Guardrails & reliability",
          "goto": "n_rag_guardrails_01"
        },
        {
          "id": "o_rag_deploy",
          "label": "Serving & latency",
          "goto": "n_rag_serving_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_rag_retrieval_01",
      "type": "answer",
      "section": "rag",
      "bot": "I treat retrieval as the core: good chunking, metadata, hybrid signals when needed, and reranking if it improves precision. If retrieval is weak, the chatbot is a liar no matter how good the model is.",
      "options": [
        {
          "id": "o_rag_guardrails2",
          "label": "What guardrails do you use?",
          "goto": "n_rag_guardrails_01"
        },
        {
          "id": "o_rag_back",
          "label": "Back",
          "goto": "n_rag_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_rag_guardrails_01",
      "type": "answer",
      "section": "rag",
      "bot": "Guardrails: intent routing, threshold gating, safe fallbacks when evidence is weak, and structured outputs. The goal is to fail safely instead of hallucinating confidently.",
      "options": [
        {
          "id": "o_rag_serving2",
          "label": "Serving & latency",
          "goto": "n_rag_serving_01"
        },
        {
          "id": "o_rag_back2",
          "label": "Back",
          "goto": "n_rag_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_fast_01",
      "type": "answer",
      "section": "fast",
      "bot": "60-second overview: I build production-ready ML systems across Computer Vision, RAG chatbots, time series forecasting, and MLOps. Flagship proof: (1) camera-based autonomous driving thesis pipeline, (2) IEEE UAV solar defect detection pipeline, (3) real-world adaptive traffic forecasting + scaling to 45+ intersections. If you want depth, pick any project.",
      "options": [
        {
          "id": "o_fast_projects",
          "label": "Pick a project",
          "goto": "n_proj_01"
        },
        {
          "id": "o_fast_contact",
          "label": "Contact",
          "goto": "n_contact_01",
          "utterance": "How can I contact you, and where can I see your work?"
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "CV (PDF)",
          "src": "CV_Georgios_Terzoglou.pdf"
        },
        {
          "type": "link",
          "label": "GitHub",
          "src": "https://github.com/Giorgos-Terzoglou"
        },
        {
          "type": "link",
          "label": "LinkedIn",
          "src": "https://www.linkedin.com/in/georgios-terzoglou-792722348/"
        }
      ]
    },
    {
      "id": "n_contact_01",
      "type": "answer",
      "section": "contact",
      "bot": "Contact: email is best. You can also check GitHub and LinkedIn. CV download is available.",
      "options": [
        {
          "id": "o_contact_projects",
          "label": "Back to projects",
          "goto": "n_proj_01"
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "Email",
          "src": "mailto:gioterzoglou@gmail.com"
        },
        {
          "type": "link",
          "label": "CV (PDF)",
          "src": "CV_Georgios_Terzoglou.pdf"
        },
        {
          "type": "link",
          "label": "GitHub",
          "src": "https://github.com/Giorgos-Terzoglou"
        },
        {
          "type": "link",
          "label": "LinkedIn",
          "src": "https://www.linkedin.com/in/georgios-terzoglou-792722348/"
        }
      ]
    },
    {
      "id": "n_proj_auto_lane_limits_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Lane detection — hard parts & limitations\n\n• Pure camera + classical CV (colors/edges/lines) breaks on worn lanes, shadows, glare, and overexposure.\n• Greece-style roads: missing/dirty lane markings and inconsistent paint → unstable line fitting.\n• Curves/merges/exits confuse simple geometric assumptions.\n• Result: accuracy is demo-grade; needs stronger learning-based lane models + better temporal tracking.",
      "options": [
        {
          "id": "o_lane_back_overview",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_lane_back_modules",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Ok! Let's discuss anothor project."
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Lane → steering suggestion",
          "src": "autonomous_driving/lane_detection&steering_advice.png"
        }]
    },
    {
      "id": "n_proj_auto_signs_limits_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Traffic signs / lights — hard parts & limitations\n\n• Small-object problem: far signs are tiny → detector misses or misclassifies.\n• Weather/lighting: night, rain, fog, glare, backlight, motion blur reduce reliability.\n• Occlusions and clutter (trees, poles, ads) create false positives.\n• Needs stronger detection + data diversity + temporal consistency to be robust.",
      "options": [
        {
          "id": "o_signs_back_overview",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_signs_back_modules",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Ok! Let's discuss anothor project."
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Traffic sign detection - small objects",
          "src": "autonomous_driving/signs_detection.png"
        },
        {
          "type": "image",
          "label": "Night detection challenges",
          "src": "autonomous_driving/night_dificult.png"
        }
      ]
    },
    {
      "id": "n_proj_auto_dist_limits_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Detection + distance — hard parts & limitations\n\n• Monocular distance is a heuristic: bbox size changes with camera angle, FOV, and partial occlusion.\n• Different object sizes (car vs truck vs bike) violate the “nominal size” assumption.\n• Without calibration and depth cues, estimates drift and can be dangerously wrong.\n• Fix = calibrated camera model + learning-based depth / stereo / sensor fusion.",
      "options": [
        {
          "id": "o_dist_back_overview",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_dist_back_modules",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Ok! Let's discuss anothor project."
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Detection + distance estimation",
          "src": "autonomous_driving/detection&distance.png"
        },
        {
          "type": "video",
          "label": "Signs + distance estimation (demo)",
          "src": "autonomous_driving/trafficSigns_and_distances.mp4"
        }
      ]
    },
    {
      "id": "n_proj_solar_det_limits_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "Panel detection — hard parts & limitations\n\n• Motion blur and oblique viewing angles reduce box stability.\n• Small panels at altitude become a small-object problem.\n• Bright reflections and low contrast backgrounds can create false positives.\n• Errors here propagate downstream, which is why later stages add consistency checks.",
      "options": [
        {
          "id": "o_solar_det_backstages",
          "label": "Back to stages",
          "utterance": "Show me the stages again.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_lane_back_modules",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Ok! Let's discuss anothor project."
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Panel detection",
          "src": "solar_panel_defects/panel_detection.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_seg_limits_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "Panel interior isolation — hard parts & limitations\n\n• UNet can fail under extreme perspective, occlusions, or strong glare.\n• Borders/frames and background clutter can leak into the mask.\n• RGB–thermal misalignment makes “clean interior” harder.\n• The CV fallback exists because real UAV capture is messy and you need a plan B.",
      "options": [
        {
          "id": "o_solar_seg_backstage",
          "label": "Back to pipeline stages",
          "utterance": "Back to the pipeline stages.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_backmenu_seg",
          "label": "Back to projects",
          "utterance": "Ok! Let's discuss anothor project.",
          "goto": "n_proj_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Segmentation pipeline",
          "src": "solar_panel_defects/panel_segmentation.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_cls_limits_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "Fault classification — hard parts & limitations\n\n• Rare defect types lead to data imbalance; some classes needed augmentation.\n• Thermal noise + capture drift can create patterns that look like defects.\n• Without explainability, a classifier can be confidently wrong.\n• That’s why the pipeline includes heatmaps + thermal sanity checks instead of pure labels.",
      "options": [
        {
          "id": "o_solar_seg_backstage",
          "label": "Back to pipeline stages",
          "utterance": "Back to the pipeline stages.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_backmenu_seg",
          "label": "Back to projects",
          "utterance": "Ok! Let's discuss anothor project.",
          "goto": "n_proj_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Thermal → heatmap",
          "src": "solar_panel_defects/panel_classification.png"
        }
      ]
    },
    {
      "id": "n_proj_auto_seg_limits_01",
      "type": "answer",
      "section": "projects.autonomous_driving",
      "bot": "Drivable-area segmentation — hard parts & limitations\n\n• Road appearance variance (asphalt quality, patches, dirt) and harsh lighting break segmentation.\n• Rural/Greek roads without clear boundaries or markings are especially hard.\n• Shadows, puddles, and worn edges create mask noise.\n• Needs domain-specific data + temporal smoothing + uncertainty-aware outputs.",
      "options": [
        {
          "id": "o_seg_back_overview",
          "label": "Another module",
          "goto": "n_proj_auto_perception_01",
          "utterance": "Show me another module."
        },
        {
          "id": "o_seg_back_modules",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Ok! Let's discuss anothor project."
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Drivable area segmentation",
          "src": "autonomous_driving/drivable_area_segmentation.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_segm_robust_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "Segmentation is robust here because it’s not a single point of failure.\n\n1) We trained a UNet encoder–decoder on a dedicated segmentation dataset (detector-style crops + mask pairs), so the model learns the panel interior rather than background clutter.\n\n2) We don’t trust the raw mask output. We convert it into geometry with a deterministic CV post-process (Canny → contours → 4-corner check → perspective warp), which standardizes every panel into the same view.\n\n3) When the UNet prediction is noisy, we recover using a classical vision fallback (HoughLinesP → line extension → contours + warpPerspective). That fallback is why we consistently keep the true panel interior in challenging cases instead of losing the panel or keeping background.",
      "options": [
        {
          "id": "o_solar_seg_backstage",
          "label": "Back to pipeline stages",
          "utterance": "Back to the pipeline stages.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_backmenu_seg",
          "label": "Back to projects",
          "utterance": "Ok! Let's discuss anothor project.",
          "goto": "n_proj_01"
        }
      ],
      "evidence": [
        {
          "type": "image",
          "label": "Segmentation pipeline",
          "src": "solar_panel_defects/panel_segmentation.png"
        },
        {
          "type": "image",
          "label": "Fallback pipeline view",
          "src": "solar_panel_defects/panel_detection_full_pipeline.png"
        }
      ]
    },
    {
      "id": "n_proj_solar_limits_01",
      "type": "answer",
      "section": "projects.solar_panel_defects",
      "bot": "System limitations (by design, not by accident)\n\n• Data availability: some fault types are rare in real solar parks, which required synthetic data generation to train the classifier.\n• Capture conditions: UAV altitude, viewing angle, motion blur, and thermal noise directly affect detection and segmentation quality.\n• RGB–thermal alignment: small misalignments propagate into segmentation and thermal statistics, requiring careful preprocessing.\n• Not a turnkey product: deployment still requires calibration, domain adaptation, and flight protocol discipline.\n\nThese constraints are typical for real UAV inspection systems — the pipeline is designed to acknowledge them, not hide them.",
      "options": [
        {
          "id": "o_solar_seg_backstage",
          "label": "Pipeline stages",
          "utterance": "Let's speak about the pipeline stages.",
          "goto": "n_proj_solar_pipeline_01"
        },
        {
          "id": "o_solar_backmenu_seg",
          "label": "Back to projects",
          "utterance": "Ok! Let's discuss anothor project.",
          "goto": "n_proj_01"
        }
      ],
      "evidence": []
    },
    {
      "id": "n_proj_interpreter_overview_01",
      "type": "answer",
      "section": "projects.interpreter",
      "bot": "INTERPRETER (H2020) — smart grid management tools for DSOs/TSOs.\n\nMy contribution:\n• Built tabular ML + time-series analysis to predict when transformers / assets are likely to degrade.\n• Implemented optimization logic to prioritize actions under constraints (budget, risk, operational limits).\n• Designed retraining triggers so performance doesn’t silently decay as grid behavior drifts.\n\nProof is the project site — no hand-wavy claims.",
      "options": [
        {
          "id": "o_interpreter_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "INTERPRETER website",
          "src": "https://www.interpreter-h2020.eu/"
        }
      ]
    },
    {
      "id": "n_proj_mementoes_overview_01",
      "type": "answer",
      "section": "projects.mementoes",
      "bot": "MEMENTOES — AI services for cultural heritage: improving imagery and enabling better 3D reconstruction for digital twins.\n\nMy contribution:\n• Image-to-image enhancement: style transfer + super-resolution for degraded/low-res visual material.\n• Support for 3D reconstruction pipelines (cleaner inputs → better geometry/texture output).\n• Active-learning style iteration to improve results faster (targeted data selection instead of brute-force labeling).\n• Packaged components as services so the stack is accessible and reusable, not trapped in notebooks.\n\nProof is the project site + the two publications.",
      "options": [
        {
          "id": "o_mementoes_pub_elsevier",
          "label": "Publication: Digital Heritage Digital Twins (Elsevier)",
          "goto": "n_pub_mementoes_digital_twins_01",
          "utterance": "Can you please tell me more about the digital heritage digital twins publication."
        },
        {
          "id": "o_mementoes_pub_ieee_game",
          "label": "Publication: Customizable Game Assets (IEEE)",
          "goto": "n_pub_game_assets_01",
          "utterance": "Can you please tell me more about the IEEE paper about customizable game assets."
        },
        {
          "id": "o_mementoes_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "MEMENTOES website",
          "src": "https://mementoes.eu/#"
        }
      ]
    },
    {
      "id": "n_pub_game_assets_01",
      "type": "answer",
      "section": "publications",
      "bot": "Publication — AI Services for Generating Customizable Game Assets (IEEE)\n\nWhat it’s about\nA suite of user-friendly AI services that let developers turn short real-world captures into customizable game-ready assets — especially for objects of historical importance.\n\nWhat the system provides\n• enhancement + semantic enrichment services for 2D images and videos\n• 3D asset acquisition via video-based 3D reconstruction\n• parameter-driven customization so developers can steer the output (not a one-size-fits-all generator)\n\nWhy it matters\nIt lowers the cost of digitizing real objects into interactive content, while improving immersion and enabling serious games / educational applications.\n",
      "options": [
        {
          "id": "o_pub_game_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        },
        {
          "id": "o_pub_game_back_mementoes",
          "label": "Back to MEMENTOES",
          "goto": "n_proj_mementoes_overview_01",
          "utterance": "Back to MEMENTOES."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "IEEE link",
          "src": "https://ieeexplore.ieee.org/abstract/document/10976973"
        }
      ]
    },
    {
      "id": "n_pub_mementoes_digital_twins_01",
      "type": "answer",
      "section": "publications",
      "bot": "Publication — Digital twins pipeline for cultural heritage (image enhancement + 3D asset creation).\n\nWhat the work delivers:\n• A semi-automated pipeline to acquire and enhance visual material, then build higher-quality 3D assets.\n• Super-resolution + style transfer used to improve the frames that feed multi-view 3D reconstruction.\n• Active-learning loop: generate/select new views and iterate to improve the final 3D result with fewer wasted cycles.\n\nThis is not “cool demos”. It’s a structured pipeline with measurable improvements.",
      "options": [
        {
          "id": "o_pub_dt_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        },
        {
          "id": "o_pub_dt_back_mementoes",
          "label": "Back to MEMENTOES",
          "goto": "n_proj_mementoes_overview_01",
          "utterance": "Back to MEMENTOES."
        }
      ],
      "evidence": [
        {
          "type": "link",
          "label": "ScienceDirect link",
          "src": "https://www.sciencedirect.com/science/article/pii/S2212054825000530"
        },
        {
          "type": "image",
          "label": "Workflow",
          "src": "mementoes/digital_heritage/workflow.png"
        },
        {
          "type": "image",
          "label": "3d reconstruction with style transfer + super-resolution",
          "src": "mementoes/digital_heritage/3d_recon.png"
        },
        {
          "type": "image",
          "label": "Style transfer and super-resolution",
          "src": "mementoes/digital_heritage/style_transfer_sr.png"
        }

      ]
    },
    {
      "id": "n_pub_solar_dsp_01",
      "type": "answer",
      "section": "publications",
      "bot": "Publication — Employing deep learning framework for improving solar panel defects using drone imagery (DSP 2023 / IEEE).\n\nWhat it does (multi-stage pipeline)\n• Detect panels in UAV RGB/thermal imagery with YOLOv5.\n• Isolate the panel interior with segmentation (UNet + classic CV fallback when the mask fails).\n• Classify panels as normal vs anomalous and then into fault types (cell, multi-cell, diode, multi-diode) using EfficientNet trained on synthetic faults.\n• Add a thermal-statistics ML stage to flag defects missed by vision-only steps (high class-imbalance handled with SMOTE; Random Forest used as the best-performing baseline).\n\nWhat’s measured (not hand-waving)\n• Panel detection succeeds on test imagery at ~90% confidence.\n• Interior isolation succeeds on ~87% of test images (with fallback logic).\n• Classification reports strong F1 for no-fault and solid average F1 across fault classes.\n\nBottom line: it’s an end-to-end inspection pipeline designed for real drone capture noise — not a single-model demo.",
      "options": [
        {
          "id": "o_pub_solar_dsp_back_pubs",
          "label": "Back to publications",
          "goto": "n_pubs_01",
          "utterance": "Back to publications."
        },
        {
          "id": "o_pub_solar_dsp_back_projects",
          "label": "Back to projects",
          "goto": "n_proj_01",
          "utterance": "Back to projects."
        },
        {
          "id": "o_pub_solar_dsp_back_pvgnosis",
          "label": "Back to PVGNOSIS",
          "goto": "n_proj_solar_overview_01",
          "utterance": "Back to PVGNOSIS."
        }

      ],
      "evidence": [
        {
          "type": "link",
          "label": "DSP 2023 paper (PDF)",
          "src": "https://2025.ic-dsp.org/wp-content/uploads/2023/05/DSP2023-97.pdf"
        },
        {
          "type": "link",
          "label": "IEEE entry",
          "src": "https://ieeexplore.ieee.org/document/10167960"
        }
      ]
    }




  ]
}